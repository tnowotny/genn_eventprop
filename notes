older (random) notes:
---
- check shuffling for speaker x-validation




Progress notes
---

scan_JUWELS_7 was run with exponential weighting of voltages in the loss function when the choice was not yet explicit

Scan_JUWELS_8 to 10 were run with the standard x-entropy of integral loss function in an (unsuccessful) attempt of reproducing CNS poster results. It is unclear which detail is missing, maybe the batch size?

Scan_JUWELS_11 is run with linear weighting in the V intgrals ... now an explicit choice

(at this point it seems the weighted versions not only learn faster but also somewhat better (though below the 80% ALIF e-prop ...)

MNIST works with first spike loss when using the output neuron regularisation with 1/x^2 and alpha= 3e-3
NOTE: It does still spawn a good number of phantom spikes but it does not seem to matter
With the original Wunderlich/Pehle regularisation term on the output, MNIST does not seem to work; maybe one needs a full parameter optimisation on the three parameters tau_0, alpha and tau_1. Maybe not worth the effort.
For now the original term remains commented out and the 1/x^2 term remains as the favourite solution.

---

reconstructing earlier success with plain "sum" loss function:
very important: the regularisation strength needs to be really small!! I think it translates to 4e-15 in the current code formulation (not divided by N_batch)
which translates to 1e-12 if dividing by N_batch. Comparing output neuron Lambda updates with regularisation updates, it probably makes sense to divide by N_batch in the regularisation as well to eb comparable with the output neuron updates that are divided by N_batch.
-> revert back to dividing and hence lbd_upper/lbd_lower values around 1e-12

*** lbd_upper/lbd_lower values of other runs may have to be multiplied by N_BATCH (YUCK!) - especially scan_juwels_7 (or earlier?) - scan_juwels_15.


2022-10-14:
will rescale the regularisation by 1/N_batch. This will need rescaling of LBD_UPPER, LBD_LOWER by N_batch compared to previous runs.






